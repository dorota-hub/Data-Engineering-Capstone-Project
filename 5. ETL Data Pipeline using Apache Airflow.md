A pipeline that analyzes the web server log file, extracts the required lines(ending with html) and fields(time stamp, size ) and transforms (bytes to mb) and load (append to an existing file)

# Create a DAG
```console
touch process_web_log.py
```

```python
#imports
from datetime import timedelta
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.utils.dates import days_ago
```
## Define the DAG arguments
```python
default_args = {
    'owner': 'Dorota Facon-Dziubak',
    'start_date': days_ago(0),
    'email': ['dorotafacon@gmail.com'],
    'retry_delay': timedelta(minutes=5),
}
```
## Define the DAG
```python
dag = DAG(
    'process_web_log',
    default_args=default_args,
    description = 'Daily proceessing of the web server log file',
    schedule_interval=timedelta(days=1),
)
```

## Extract data from a web server log file
```python
extract_data = BashOperator(
    task_id = 'extract_data',
    bash_command = 'cut -f1 -d" " accesslog.txt > /home/project/airflow/dags/capstone/extracted_data.txt',
    dag=dag,
)
```

## Transform the data
```python
transform_data = BashOperator(
    task_id = 'transform_data',
    bash_command = 'grep -vw "198.46.149.143" < /home/project/airflow/dags/capstone/extracted_data.txt > /home/project/airflow/dags/capstone/transformed_data.txt',
    dag=dag,
)
```

## Load the transformed data into a tar file
```python
load_data = BashOperator(
    task_id = 'load_data',
    bash_command = 'tar -cf weblog.tar transformed_data.txt',
    dag=dag,
)
```
## Define the task pipeline
```python
extract_data >> transform_data >> load_data
```

# Getting the DAG operational
## Submit the DAG
```console
cd process_web_log.py > $AIRFLOW_HOME/dags
```
## Unpause the DAG
```console
airflow dags unpause process_web_log
```

